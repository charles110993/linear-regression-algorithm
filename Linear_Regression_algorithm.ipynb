{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "iOD62u1j3dwZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(X):\n",
        "    # Perform any necessary preprocessing steps on the feature matrix X\n",
        "    # For example, handle missing values, convert categorical variables, etc.\n",
        "    # This function should return the preprocessed feature matrix\n",
        "    # Example: Data normalization\n",
        "    X_normalized = (X - X.min()) / (X.max() - X.min())\n",
        "    return X_normalized"
      ],
      "metadata": {
        "id": "kNCPcMte3hQ8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_bias(X):\n",
        "    # Add a column of ones to the feature matrix X for the bias term\n",
        "    return np.c_[np.ones(X.shape[0]), X]\n"
      ],
      "metadata": {
        "id": "zU8FYXSc3pGa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cost(X, y, weights):\n",
        "    # Calculate the mean squared error cost function\n",
        "    predictions = X.dot(weights)\n",
        "    error = predictions - y\n",
        "    cost = np.mean(error ** 2) / 2\n",
        "    return cost"
      ],
      "metadata": {
        "id": "-KNJ987E3uYi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, learning_rate, num_iterations):\n",
        "    # Perform gradient descent to optimize the weights\n",
        "    num_features = X.shape[1]\n",
        "    weights = np.zeros(num_features)\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        predictions = X.dot(weights)\n",
        "        error = predictions - y\n",
        "        gradient = X.T.dot(error) / len(X)\n",
        "        weights -= learning_rate * gradient\n",
        "\n",
        "        cost = calculate_cost(X, y, weights)\n",
        "        costs.append(cost)\n",
        "\n",
        "    return weights, costs"
      ],
      "metadata": {
        "id": "4wM7CL-h3z6R"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_cost_vs_iterations(costs):\n",
        "    # Plot the cost function value vs. number of iterations\n",
        "    plt.plot(range(len(costs)), costs)\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Cost')\n",
        "    plt.title('Cost vs. Iterations')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "SSoSjWst38VL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_regression_line(X, y, weights):\n",
        "    # Plot the regression line\n",
        "    plt.scatter(X[:, 1], y, color='blue', label='Actual')\n",
        "    plt.plot(X[:, 1], X.dot(weights), color='red', label='Predicted')\n",
        "    plt.xlabel('Feature')\n",
        "    plt.ylabel('Output')\n",
        "    plt.title('Linear Regression')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nnYxEayX4BZq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression(x_train, y_train, learning_rate=0.01, num_iterations=1000):\n",
        "    # Perform Linear Regression on the training data\n",
        "    X = preprocess_data(x_train)\n",
        "    X = add_bias(X)\n",
        "    y = y_train\n",
        "\n",
        "    weights, costs = gradient_descent(X, y, learning_rate, num_iterations)\n",
        "\n",
        "    plot_cost_vs_iterations(costs)\n",
        "    plot_regression_line(X, y, weights)\n",
        "\n",
        "    return weights\n",
        "\n",
        "# Load the dataset\n",
        "dataset = \"Medical Price Dataset.csv\"\n",
        "data = pd.read_csv(dataset)\n",
        "\n",
        "# Print the column names\n",
        "print(data.columns)\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = data[['age', 'bmi', 'children']]\n",
        "y_train = data['charges']\n",
        "\n",
        "weights = linear_regression(x_train, y_train)\n",
        "\n",
        "print(\"Weights:\", weights)\n"
      ],
      "metadata": {
        "id": "fhQTRkjk4LIX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}